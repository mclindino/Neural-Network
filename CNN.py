# -*- coding: utf-8 -*-
"""CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RxfWipsCEdVJfq9Q--r43U2dYEUUxkxU
"""

# %matplotlib inline

import numpy as np
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
from torch.utils.data import DataLoader

import torchvision
import torchvision.transforms as transforms

import time

transform_greyscale = transforms.Compose([
    transforms.Grayscale(num_output_channels = 1),
    transforms.ToTensor()
])

transform_RGB = transforms.Compose([
    transforms.ToTensor()
])

dataset_train_greyscale = torchvision.datasets.CIFAR10(root ='/data', train = True,
                                            download=True ,transform=transform_greyscale)

dataset_test_greyscale = torchvision.datasets.CIFAR10(root='/data', train=False,
                                           download=True, transform=transform_greyscale)

dataset_train_RGB = torchvision.datasets.CIFAR10(root='/data', train=True,
                                                download=True, transform=transform_RGB)

dataset_test_RGB = torchvision.datasets.CIFAR10(root='/data', train=False,
                                                download=True, transform=transform_RGB)

train_loader_greyscale = DataLoader(dataset=dataset_train_greyscale, shuffle=True)
test_loader_greyscale = DataLoader(dataset=dataset_test_greyscale, shuffle=False)

train_loader_RGB = DataLoader(dataset=dataset_train_RGB, shuffle=True)
test_loader_RGB = DataLoader(dataset=dataset_test_RGB, shuffle=False)

class CNN(nn.Module):
  def __init__(self, RGB):
    super(CNN, self).__init__()
    self.RGB = RGB
    if RGB:
      self.conv1 = nn.Conv2d(3, 18, kernel_size=3, stride=1, padding=1)
      self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)
    else:
      self.conv1 = nn.Conv2d(1, 18, kernel_size=3, stride=1, padding=1)
      self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)
    
    
    self.fc1 = nn.Linear(18*16*16, 64)
    self.drop = nn.Dropout(0.6)
    self.fc2 = nn.Linear(64, 10)
    
    self.activation_function = nn.ReLU()
    
  def forward(self, x):
    x = self.activation_function(self.pool1(self.conv1(x)))
    x = x.view(-1, 18*16*16)
    x = self.activation_function(self.fc1(x))
    x = self.drop(x)
    x = self.activation_function(self.fc2(x))
    
    return x

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
CNN_greyscale = CNN(False)
CNN_greyscale.to(device)

def create_Loss_Optimizer(model, learning_rate):
  loss = nn.CrossEntropyLoss()
  optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)  
  return (loss, optimizer)

def trainCNN(model, train_dataset, n_epochs, learning_rate):
  loss, optimizer = create_Loss_Optimizer(model, learning_rate)
  training_start_time = time.time()
  loss_vector = []
  accuracy_vector = []
  for epoch in range(n_epochs):
    start_time = time.time()
    total_loss = 0
    correct = 0
    model.train()
    for i, (img, label) in enumerate(train_dataset, 0):
      img, label = img.to(device), label.to(device)
      if(i < 35000):
        optimizer.zero_grad()
        output = model(img)
        loss_size = loss(output, label)
        loss_size.backward()
        optimizer.step()
      
      else:
        model.eval()
        output = model(img)
        loss_size = loss(output, label)
        total_loss += loss_size.item()
        _, pred = torch.max(output.data, 1)
        correct += (pred == label).sum().item()
    
    total_loss = total_loss / 15000
    loss_vector.append(total_loss)
    accuracy = 100 * correct / 15000
    accuracy_vector.append(accuracy)
    print('Epoch {} => \tTrain_loss: {:.2f}\tTempo: {:.2f}s\tAcurácia_validação: {:.2f}%'.format(epoch+1, loss_size, time.time() - start_time, accuracy))
    start_time = time.time()
        
  print('Training finished, took: {}'.format(time.time() - training_start_time))
  return (loss_vector, accuracy_vector)

def testCNN(model, test_dataset):
  model.eval()
  correct = 0
  for img, label in test_dataset:
    img, label = img.to(device), label.to(device)
    output = model(img)
    _, pred = torch.max(output.data, 1)
    correct += (pred == label).sum().item()
  accuracy = 100 * correct / len(test_dataset.dataset)
  print('Acurácia teste: {:.2f}%'.format(accuracy))

loss_vector, accuracy_vector = trainCNN(CNN_greyscale, train_loader_greyscale, 50, 0.001)

testCNN(CNN_greyscale, test_loader_greyscale)

print('VALIDAÇÃO:\n')
plt.title('Loss - GRAYSCALE')
plt.plot(loss_vector)
plt.show()

plt.title('Accuracy (%) - GRAYSCALE')
plt.plot(accuracy_vector)
plt.show()

"""#Imagens coloridas:"""

CNN_RGB = CNN(True)
CNN_RGB.to(device)

loss_vector, accuracy_vector = trainCNN(CNN_RGB, train_loader_RGB, 50, 0.001)

testCNN(CNN_RGB, test_loader_RGB)

print('VALIDAÇÃO:\n')
plt.title('Loss - RGB')
plt.plot(loss_vector)
plt.show()

plt.title('Accuracy (%) - RGB')
plt.plot(accuracy_vector)
plt.show()

"""#Conclusões


*   Utilizando apenas um modulo, que consiste em uma camada Convolucional e uma Max Pool, conseguiu um desempenho de 59% de acurácia nas imagens em escala de cinza, e 60% de acurácia nas imagens coloridas.

*   Com o Batch Normalization, os resultados obtidos na validação foram mais próximos aos resultados encontrados na fase de teste, porém demorou bem mais épocas para chegar aos 57%, então, por questão de tempo, optou-se a não utilizar.

*   O melhor learning rate observado foi de lr=0.001, não foi utilizado momentum.

*   Pelos os gráficos,  a tendência é de aumentar e estabilizar a acurácia perto dos 60% na escala de cinza e 63% nas imagens RGB. 

*   Nesta atividade, a rede neural conseguiu com maior êxito classificar as imagens no padrão RGB.
"""