# -*- coding: utf-8 -*-
"""MLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NzOYhKYMrn_EDJbGBWdsyV4hyv8QqaTh
"""

# %matplotlib inline

import numpy as np
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
from torch.utils.data import DataLoader

import torchvision
import torchvision.transforms as transforms

transform = transforms.Compose([
	transforms.Grayscale(num_output_channels = 1),
	transforms.ToTensor()
])

dataset_train = torchvision.datasets.CIFAR10(root='./data', train=True,
											download=True, transform=transform)

dataset_test = torchvision.datasets.CIFAR10(root='./data', train=False,
											download=True, transform=transform)

train_loader = DataLoader(dataset=dataset_train, batch_size = 200, shuffle=True)
test_loader = DataLoader(dataset=dataset_test, shuffle=False)

class MLP(nn.Module):
  def __init__(self, n_hidden_nodes, n_hidden_layers):
    super(MLP, self).__init__()
    self.n_hidden_nodes = n_hidden_nodes
    self.n_hidden_layers = n_hidden_layers
    
    self.fc1 = nn.Linear(32*32, n_hidden_nodes)
    self.fc1_drop = nn.Dropout(0.5)
    if n_hidden_layers == 2:
      self.fc2 = nn.Linear(n_hidden_nodes, n_hidden_nodes)
      self.fc2_drop = nn.Dropout(0.5)
      
    self.out = nn.Linear(n_hidden_nodes, 10)
    self.activation_function = nn.ReLU()
    
  def forward(self, x):
    x = x.view(-1, 32*32)
    x = self.activation_function(self.fc1(x))
    x = self.fc1_drop(x)
    if self.n_hidden_layers == 2:
      x = self.activation_function(self.fc2(x))
      x = self.fc2_drop(x)
    x = self.activation_function(self.out(x))
    return x

mlp = MLP(20, 1) #MLP 1 hidden layer com 20 hidden node
optimizer = torch.optim.SGD(mlp.parameters(), lr = 0.01, momentum=0.1)
loss_fn = torch.nn.CrossEntropyLoss()

def train_model(epochs, train_dataset, model, loss_vector, accuracy_vector):
  correct = 0
  model.train()
  for i, (img, labels) in enumerate(train_dataset, 0):
      optimizer.zero_grad()
      output = model(img)
      loss = loss_fn(output, labels)
      loss_vector.append(loss)
      loss.backward()
      optimizer.step()
      
      model.eval()
      output = model(img)
      _, pred = torch.max(output.data, 1)
      correct += (pred == labels).sum().item()
  
  accuracy = 100 * correct / len(train_dataset.dataset)
  accuracy_vector.append(accuracy)
  print('Train Epoch: {}\t Accuracy: {}/{} [{}%]'.format(epochs, correct, len(train_dataset.dataset), accuracy))

def test(model, test_loader):
  correct = 0
  for img, label in test_loader:
    output = model(img)
    _, pred = torch.max(output.data, 1)
    correct += (pred == label).sum().item()
  
  accuracy = 100 * correct / len(test_loader.dataset)
  print('\nTest: \tAccuracy: {}/{} ({}%)\n'.format(correct, len(test_loader.dataset), accuracy))

loss_vector = []
accuracy_vector = []
for epoch in range(100):
  train_model(epoch, train_loader, mlp, loss_vector, accuracy_vector)

test(mlp, test_loader)

plt.plot(loss_vector)
plt.show()

plt.plot(accuracy_vector)
plt.show()

"""#Discussão


*   A classe MLP é configurável. Pode-se decidir entre 1 ou 2 camadas ocultas, e a quantidade de neuronios em cada uma delas. Em cada camada, tem a função Dropout.
*   Consegue-se observar uma melhora significativa do perceptron, já que esse ultimo teve uma acurácia de 12%.
*   A divisa de treinamento e validação, com as mesmas configurações, resultou em um resultado de 13% em 200 épocas. Por isso, não foi divido para melhor análise de loss.
*   Utilizando 2 camadas, o treinamento me deu a informação de loss e acurácia mais rápido, porem demorou bem mais para ter uma taxa de acerto boa. Precisaria de mais épocas para ter resultados bons.
"""